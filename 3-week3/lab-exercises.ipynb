{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1:**\n",
    "\n",
    "Last week, we started looking at the Titanic data.  Load that data again. For features that you think are of minimal importance and / or have too many NAs to make imputation feasible, drop those features.  For those that seem important, use a `SimpleImputer` to impute the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pclass                                             name     sex  \\\n",
      "0          1                    Allen, Miss. Elisabeth Walton  female   \n",
      "1          1                   Allison, Master. Hudson Trevor    male   \n",
      "2          1                     Allison, Miss. Helen Loraine  female   \n",
      "3          1             Allison, Mr. Hudson Joshua Creighton    male   \n",
      "4          1  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
      "...      ...                                              ...     ...   \n",
      "1304       3                             Zabour, Miss. Hileni  female   \n",
      "1305       3                            Zabour, Miss. Thamine  female   \n",
      "1306       3                        Zakarian, Mr. Mapriededer    male   \n",
      "1307       3                              Zakarian, Mr. Ortin    male   \n",
      "1308       3                               Zimmerman, Mr. Leo    male   \n",
      "\n",
      "          age  sibsp  parch  ticket      fare    cabin embarked  survived  \n",
      "0     29.0000      0      0   24160  211.3375       B5        S         1  \n",
      "1      0.9167      1      2  113781  151.5500  C22 C26        S         1  \n",
      "2      2.0000      1      2  113781  151.5500  C22 C26        S         0  \n",
      "3     30.0000      1      2  113781  151.5500  C22 C26        S         0  \n",
      "4     25.0000      1      2  113781  151.5500  C22 C26        S         0  \n",
      "...       ...    ...    ...     ...       ...      ...      ...       ...  \n",
      "1304  14.5000      1      0    2665   14.4542      NaN        C         0  \n",
      "1305      NaN      1      0    2665   14.4542      NaN        C         0  \n",
      "1306  26.5000      0      0    2656    7.2250      NaN        C         0  \n",
      "1307  27.0000      0      0    2670    7.2250      NaN        C         0  \n",
      "1308  29.0000      0      0  315082    7.8750      NaN        S         0  \n",
      "\n",
      "[1309 rows x 11 columns]\n",
      "pclass         0\n",
      "name           0\n",
      "sex            0\n",
      "age          263\n",
      "sibsp          0\n",
      "parch          0\n",
      "ticket         0\n",
      "fare           1\n",
      "cabin       1014\n",
      "embarked       2\n",
      "survived       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tdf=pd.read_csv(\"3-week3/data/titanic.csv\")\n",
    "print(tdf)\n",
    "\n",
    "\n",
    "print(tdf.isnull().sum())\n",
    "#this shows me that cabin and embarked have a lot of nulls- i'll drop.\n",
    "#impute age and fare\n",
    "\n",
    "tdf = tdf.drop('embarked', axis=1)\n",
    "tdf = tdf.drop('cabin', axis=1)\n",
    "tdf['age'] = tdf['age'].fillna(tdf['age'].mean())\n",
    "tdf['fare'] = tdf['fare'].fillna(tdf['fare'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass      0\n",
      "name        0\n",
      "sex         0\n",
      "age         0\n",
      "sibsp       0\n",
      "parch       0\n",
      "ticket      0\n",
      "fare        0\n",
      "survived    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tdf.isnull().sum()) #all are filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 2**\n",
    "\n",
    "Sklearn does not handle strings.  Use an encoder to transform any string columns into numbers. If there are any categorical columns where label encoding won't work, use a one-hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#use an ordinal encoder when the order of the columns matter. onehotencoder for other instances (classes)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_clean\u001b[38;5;241m=\u001b[39m\u001b[43mtdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msibsp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurvived\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m ohe\u001b[38;5;241m=\u001b[39m OneHotEncoder (drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m, sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m X\u001b[38;5;241m=\u001b[39mohe\u001b[38;5;241m.\u001b[39mfit_transform(data[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membarked\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "#use an ordinal encoder when the order of the columns matter. onehotencoder for other instances (classes)\n",
    "data_clean=tdf([\"sex\", \"age\", \"sibsp\", \"parch\",\"survived\"])\n",
    "ohe= OneHotEncoder (drop=\"first\", sparse_output=False)\n",
    "X=ohe.fit_transform(data[[\"embarked\", \"sex\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[ohe.get_feature_names_out()]=x\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean=data.clean.drop([\"sex\",\"embarked\"],axis=1)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**\n",
    "\n",
    "Using 5-fold cross-validation, examine the performance of a LogisticRegression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "lr=LogisticRegression()\n",
    "X=data_clean.drop(\"survived\",axis=1)\n",
    "y=data_clean['survived']\n",
    "\n",
    "result=cross_val_score(lr,X,y,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 4**\n",
    "\n",
    "Replace the logistic regression classifier with a decision tree classifier.  Which works better?  Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 5:** \n",
    "\n",
    "See if you can get the decision tree classifier to perform better by adjusting your imputation procedure to use a KNNImputer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 6:**\n",
    "\n",
    "Try using a `GridSearchCV` to optimize the DecisionTree algorithm.  What is the best performance you can achieve?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
